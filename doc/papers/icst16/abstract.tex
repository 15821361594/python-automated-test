Test case reduction has long been seen as essential to effective automated testing.  However, test case reduction simply reduces the \emph{length} of a test case.  It does not attempt to reduce \emph{semantic} complexity.  This paper presents algorithms for normalizing and generalizing test cases.  Re-writing test cases into a \emph{normal form} can reduce semantic complexity and, often, reduce the length of an already reduced test case.  Most importantly, normalization can convert many test cases that are due to the same underlying fault into \emph{a single test case}, reducing the number of test cases that a reader must examine, and partially addressing the ``fuzzer taming'' problem of discovering all faults in a large set of failing test cases.  Generalization, in contrast, takes a test case and reports what aspects of the test could have been changed while preserving the property that the test fails.  Together, ideally, normalization and generalization allow a user to replace reading a set of test cases that vary in unimportant ways with reading \emph{one annotated test case} (per fault) that succinctly describes a family of failures.  These algorithms rely on the features of a recently introduced DSL for testing.   We show how normalization plus generalization aids understanding of test cases, including tests for a complex and widely used commercial GIS library.  In our experiments normalization frequently reduces the number of test cases to be examined by \emph{over an order of magnitude}, and often achieves the goal of one test case per fault.