\section {Experimental Results}

This section presents some initial results of applying normalization
and generalization.  All tests were generated using pure random testing,
based on TSTL harnesses we developed previously, all included in the
TSTL release \cite{tstl}.  We also tested the
Python interface to Z3 \cite{z3}, but did not find faults thus far;
normalization did help produce more comprehensible and uniform Z3
quick tests \cite{icst2014}.

These experiments are not intended to provide a full evaluation of
normalization and generalization, but to establish the basic potential
value of
the techniques, and some initial data on core research questions.
These are:
{\bf RQ1:} How effectively does normalization reduce the number of
failures reported? {\bf RQ2:} How often does normalization lose
faults? {\bf RQ3:} What is the cost of normalization and generalization? {\bf RQ4:} How
much additional reduction over delta-debugging can normalization
provide? and {\bf RQ5:} Does normalization and generalization provide
substantial benefits in understanding complex test cases?

The primary
threat to validity is that we have only applied normalization and
generalization to tests produced using random testing for a small
number of subjects in Python (some small, some large), using our TSTL
implementation.  Note that {\bf RQ5} would need a human study to fully evaluate.

\subsection{AVLTree}

For basic experiments on {\bf RQ1-3}, we used a
simple Python implementation of AVL trees found on the web
\cite{avltree}, with 225 lines of code\footnote{All sizes non-comment, non-blank lines, by cloc \cite{cloc}.}.

\begin{figure}
\includegraphics[width=\columnwidth]{length}
\caption{Effects of normalization on AVLTree and XML parser mutants.}
\label{normeffect}
\end{figure}

Figure \ref{normeffect} shows the reduction in number of distinct
failing test cases produced by normalization and reduction (vs. reduction only) for
a set of 82 mutants \cite{mutant} of the AVLTree source code
\cite{Hunter:2007} ({\bf RQ1}).  Of the 228 mutants
produced by MutPy \cite{mutpy}, only these 82 produced at least 1 failure
in 1,000 test cases (other mutants either were equivalent or only
caused failures at a rate less than 1 in 1,000 tests).  Using only reduction, the
mean number of distinct failures for each mutant (which is, by definition,
a single fault) was 498.4 (median 485).  Using normalization,
the mean was 3.1 distinct failures (median 2).
For 38 of the 82 mutants, normalization produced only a single
representative failure.   Differences were 
statistically significant by Wilcoxon test with $p=1\times10^{-15}$.


AVLTree mutants also provided a way to evaluate the danger of
normalization losing faults, {\bf RQ2}.  Faults can be lost when normalization
changes a test case failing due to one fault into a test case failing
due to a different fault, a problem known as ``slippage''
\cite{PLDI13}.  AVLTree provides a slippage challenge, as there are
very few API calls, and all calls take the same inputs, so test cases
for faults are likely to be very close to each other in the
combinatorial space.  To test the slippage rates for normalization, we
randomly selected 364 mutant pairs drawn from the 82 detectable
mutants, and produced higher-order-mutants for each of these by
applying both mutants to the source (using only mutants that modified
different source lines).  Of these, the set of reduced test cases
included at least one test capable of exposing each of the two faults
for only 238 pairs.  In almost all cases this was due to reduction
slippage, but in a few cases it was due to one fault completely
masking another: e.g., if {\tt insert} always fails, it is not
possible to produce a test case that exposes a fault in {\tt delete}
on a non-empty tree.

Out of these 238 mutant pairs, normalization produced test cases
exposing both faults for 80.7\% of pairs (19.3\% slippage at the
suite level).  Interestingly, in 4 cases
normalization took a set of reduced test cases not capable of exposing
both faults, and produced a smaller set of test cases that was capable
of detecting both faults.  Arguably this is ``slippage'' in that a test case not
capable of exposing fault F was modified to expose fault F (classic
slippage) but since the new test suite exposed both faults,
normalization actually restored fault detection.

Data on slippage is not extensive, since it is only detected if the original test
cases before reduction are re-executed after bugs have been
fixed.  In our previous work \cite{PLDI13}, slippage due to
reduction was very rare for some SUTs (GCC 4.3.0) but common
for other SUTs (up to 23\% for Mozilla's JavaScript engine).  For the AVLTree example, the slippage rate for reduction is almost 30\%,
10\% \emph{worse} than that for normalization.  As a mitigation, we propose
storing reduced test cases (and/or unreduced test
cases) as a slippage check when all faults detected by normalized test
cases are fixed.

The reduction in test cases produced by normalization for mutant pairs
was, as with single mutants, very large (Figure \ref{normeffect}).
For mutant pairs, the mean/median number of distinct failures was
554.4/607 for reduction alone, but only 2.8/2 with
normalization.  The runtime for normalization was almost
unchanged from the single-mutant times.  For reduction
alone, using pairs increased the number of distinct failures, while
normalized failure counts decreased.  For 48.3\% (115 of 238) of
mutant pairs where reduction did not lose a fault, normalization
worked as well as possible --- it preserved both faults and produced only 1 or 2 test
cases\footnote{It is possible to detect both faults with a single test
  case, if that test case fails when \emph{either} fault is
  present.}.  The improvement due to normalization was
statistically significant by Wilcoxon test with $p=1\times10^{-80}$.

AVLTree also provided basic results for {\bf RQ3}. The mean cost to reduce a
test case was 0.05 seconds, with a median of 0.03 seconds.  The mean
cost for normalization was 0.38 seconds, with a median of 0.1 seconds.
The minimum runtime for both algorithms was negligible (less than 1
millisecond), but the maximums were 1.3 seconds for reduction and 17.6
seconds for normalization.  Note that in all our results the cost of
normalization is given on an already-reduced test case, so the inputs
for normalization are smaller than those for reduction; however, this
is the expected use-case for normalization.  Comparing on equal-sized
tests would simply involve adding the costs for reduction to those for
normalization, as an additional step of normalization.  Finally, the
criticality of caching for normalizing large numbers of tests is
evident.  Out of 60,226 normalizations performed in our full AVLTree mutant
experiments, 59,972 (99.6\%) resulted in a cache hit (most of these
after a small number of normalization steps).  In fact, the total
number of rewrites performed during the experiments was only 145,780,
for an average of only 2.4 non-cache-hit rewrites of each test.  Note
that this is with the cache starting empty for each of the 82 mutants.

\subsection{XML Parser}

We also investigated {\bf RQ1-3} for a
Python XML parser with about 260 lines of code \cite{myxml}, using one
real fault, triggered by the empty tag ({\tt <>})), and one
seeded fault triggered when adding two nodes with the same
name.  A comment in the code indicates the seeded fault is realistic,
and possibly existed in an earlier version of the code.  Running 1,000 tests
produced 848 failing test cases.  Without normalization, it took only
37.45 seconds to execute and delta-debug all 1,000 tests.  The output was 717 distinct failing test
cases.  Normalization increased the runtime to 354.7 seconds, but
reduced the number of failures to just 5: 3 
for the original fault and 2 for the seeded fault.
%Generalization took < 5 seconds.

In addition to these realistic faults, we performed the same
mutation-based analysis as with AVLTree (Figure \ref{normeffect}). In
this case, a weaker specification (no reference implementation)
resulted in fault detection for only 5 of 357 mutants.  For these
mutants, reduction alone produced an average of 107.6 failures (median
27); normalization reduced that to only 6.2 failures (median 7) ({\bf
  RQ1}).  The difference was significant at the 95\% confidence level,
by Wilcoxon test, with $p=0.042$.  With mutant pairs, these numbers
increased to 181.9 mean faults (median 175) with reduction only,
compared to 7.7 mean faults (median 7) when using normalization ({\bf
  RQ1}).  This difference was statistically significant, with Wilcoxon
$p=0.007$.  Normalization was perfect in only one case for
the XML parser mutants, and perfect for no mutant pairs.  For combined
mutants, the slippage rate was only 12.5\% for the XML parser ({\bf
  RQ2}).  Reduction alone introduced no slippage for XML parser
mutants.  Adding normalization to reduction increased the runtime by
approximately 10x, from a mean runtime of 12.6 seconds to a mean
runtime of 120.8 seconds ({\bf RQ3}); there were 3,829 cache hits over
a total of 3,929 normalizations.

\subsection{TSTL}

As noted in Section \ref{freshgen}, TSTL is used to test TSTL's own
API interface (the compiler is about 1,600 LOC; a compiled SUT is
often $>$ 30KLOC).  We discovered one fault while testing
the latest version of TSTL, the cache-related problem shown in Figure \ref{fig:mislead}.
Generating and reducing 100 test cases for it required 1,090 seconds
and produced 90 failures.  Normalization and generalization
increased total runtime to 3,690 seconds, and produced just 2
failures.

\subsection{NumPy}

Our final two case studies provide little information on {\bf RQ1} and
{\bf RQ2}; for these SUTs, failure rates are low enough or test case
reduction runtimes high enough that each failure is usually dealt with
one-by-one.  However, the value of normalization and generalization
for further reduction ({\bf RQ4}) and aiding in understanding test cases ({\bf
  RQ5}) is best shown by these complex programs.  They also provide
results for {\bf RQ3} when even simple test case reduction is expensive.
NumPy \cite{NumPy} is a widely used Python library that
supports large, multi-dimensional matrices and provides a huge library
of mathematical functions.  The SciPy library for scientific
computing builds on NumPy.  Developing tests for NumPy is challenging,
because none of the authors are experts in numeric computation, and
the specification of correct behavior is often somewhat subtle.  As a
simple example, consider the test case in Figure \ref{numpyorig},
normalized and generalized in Figure \ref{numpynormgen}.  Prior to
normalization, understanding why the test case leads to a violation of
self-equality for an array is difficult.  After
normalization, it is much clearer what is happening: 1) {\tt array0}
contains {\tt NaN} and 2) this is correct behavior (the array
\emph{should} contain {\tt NaN}).  The greater length and much larger
number of operations involved in the original reduced test case
obscures this critical point.  In NumPy, array equality does not hold
for objects containing {\tt NaN}, so the assertion must be modified.
As far as we know, normalization transforms all instances of this
fault into this canonical test case, but our data (for fewer than 30
failures) is insufficient to make a definite claim, as the failure rate
is slightly less than 3 failures/100,000 tests on average.

Other, more complex, failures have also made it clear that
normalization is useful for additional test case length reduction for
NumPy, and that generalization makes any surprising restrictions on
test values clear.  For NumPy tests, normalization takes much longer
than reduction, in part due to the expense of operations on large
arrays.  For almost all test cases, the average time to reduce tests
is about 4-5 seconds, and the time for normalization is between 712
and 774 seconds.  The average time to discover a failing test case,
for comparison, is just over 400 seconds.  Generalization takes
between 52 and 59 seconds in these cases.  The exception was a test
case of 45,206 steps (!)  leading to a memory exhaustion error and
crash.  This was reduced (over a period of nearly a day) to a test
case with 10 steps, which then normalized (in only 2 hours) to a test
case with 8 steps.  The normalized test case involved no operations other than array
initialization, array flattening, and array
addition.  The reduced test case required larger dimensions, array
multiplication, and array subtraction, as well.  This is the only case in
which we have seen normalization time lower than reduction time,
without assistance from the cache.

\begin{figure}
{\scriptsize
\begin{code}
 dim1 = 1 
 shape2 = (dim1, dim1, dim1) 
 array1 = np.ones(shape2) 
 array0 = array1 * array1 
 array1 = array1 + array1 
 array4 = array0 + array1 
 array0 = np.reshape(array4,shape2) 
 array3 = array1 * array4 
 array2 = np.ravel(array4) 
 array5 = array2 - array3 
 array4 = array5 * array2 
 array1 = np.unique(array0) 
 array5 = array5 * array3 
 array0 = array1 * array5 
 array5 = np.unique(array0) 
 array1 = array4 - array2 
 array2 = array0.flatten() 
 array0 = array5 + array5 
 array5 = array5 + array2 
 array2 = array0 * array2 
 np.copyto(array5,array2) 
 array2 = array2 * array5 
 array3 = array0 * array2 
 array0 = array3 - array1 
 array4 = array3 * array0 
 array1 = array5 + array4 
 array5 = array0 * array1 
 array0 = array5 - array1 
 array4 = array0 * array3 
 array3 = array4 * array0 
 array1 = array5 + array3 
 array0 = array2 + array1 
 array5 = array5 - array0 
 array5 = array3 * array5 
 array0 = array1 + array5 
 array2 = array3 - array0 
 array4 = array2 * array1 
 array3 = array4 * array2 
 array2 = array0 - array0 
 np.copyto(array1,array3) 
 array4 = array2.flatten() 
 array1 = array1 * array4
 assert (np.array\_equal(array1,array1))
\end{code}
}
\caption{Original ``failing'' test case for numpy (42 steps).}
\label{numpyorig}
\end{figure}

\begin{figure}
{\scriptsize
\begin{code}
dim0 = 1                            \textcolor{black!60}{\# STEP 0}
\textcolor{black!60}{\#  or dim0 = 10 }
shape0 = (dim0)                     \textcolor{black!60}{\# STEP 1}
\textcolor{black!60}{\#  or shape0 = (dim0, dim0) }
\textcolor{black!60}{\#  or shape0 = (dim0, dim0, dim0) }
array0 = np.ones(shape0)            \textcolor{black!60}{\# STEP 2}
array0 = array0 + array0            \textcolor{black!60}{\# STEP 3}
array0 = array0 + array0            \textcolor{black!60}{\# STEP 4}
\textcolor{black!60}{\#  or array0 = array0 * array0 }
array0 = array0 * array0            \textcolor{black!60}{\# STEP 5}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 6}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 7}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 8}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 9}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 10}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 11}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 12}
array0 = array0 * array0            \textcolor{black!60}{\# STEP 13}
array0 = array0 - array0            \textcolor{black!60}{\# STEP 14}
assert (np.array\_equal(array0,array0))
\end{code}
}
\caption{Normalized and generalized test case for numpy (15 steps).}
\label{numpynormgen}
\end{figure}

\subsection{Esri ArcPy}

Esri is the single
largest Geographic Information System (GIS) software vendor.  Esri's ArcGIS tools are widely
used for GIS analysis.  Automation is essential for complex GIS analysis and
data management, and Esri has long provided tools
for programming GIS software systems.  One such tool
(Esri's newest) is a Python site-package, ArcPy \cite{ArcPy}.  ArcPy is a complex library,
with dozens of classes and hundreds of functions.  Most of the code involved in ArcPy
functionality is the C++ source for ArcGIS itself (which is not
available), but the released Python interface code alone is over 50KLOC.
In order to improve the reliability of ArcPy, we are developing a
TSTL-based framework for testing ArcPy and libraries
based on ArcPy.  The ArcPy TSTL is already more than twice as large as
the next-largest TSTL harness previously studied, though it only
includes a small portion of ArcGIS functionality so far. The first
stage of testing has resulted in discovery of multiple faults in
ArcPy/ArcGIS, six of which not only cause incorrect behavior but cause
a crash that ends testing.  It is critical to understand these
faults and the behaviors that trigger them to modify the harness to
avoid triggering these faults, while restricting other testing as
little as possible.

%There is no space in this paper to elaborate on the details of this
%large test effort (which has introduced numerous additional features
%and modes to TSTL), but normalization and generalization have been
%very useful in this process.  

Figures \ref{esriorig} and
\ref{esrinormgen} show one crash-inducing test case, after initial
delta-debugging (from over 2,000 test steps) (Figure \ref{esriorig})
and after normalization and generalization (Figure \ref{esrinormgen}).
In this setting normalization has contributed a significant amount of
additional reduction over delta-debugging.  For the crash fault shown
in this paper, normalization reduced the length from 19 steps to 11
steps.  For three other crashes, normalization reduced the test cases
from 18 to 14 steps, from 27 to 20 steps, and from 20 to 16 steps. One crash fault only reduced from 10 steps to 9 steps, but the
omission was informative.  The cost of normalization is high --- in
our runs, it has taken from 17,340 seconds up to 24,769 seconds.
However, in this setting even delta-debugging is extremely expensive
--- the cost of reduction alone has ranged from 7,930 seconds to 8,688
seconds.  Generalization has taken between 3,203 and
 11,149 seconds.  These high costs are due to the need to run
tests in a sandbox environment to avoid killing the Python testing
process, and the runtime of complex GIS analyses, with individual
actions sometimes requiring minutes to execute.
Even under these circumstances, reducing, normalizing, and
generalizing test cases has been a more effective use of human time than
trying to understand the faults without these aids.  For example, in
the test case shown in this paper, it was important to understand that
the SQL query and selection type are not essential, but using a
freshly created layer will not result in a crash: the problem appears
to be that ArcGIS (or ArcPy) does not invalidate layers built from a
feature class when that feature class is deleted\footnote{In this
  instance, a generalization (the fresh values generalization in
  particular) is informative by its absence: we know that it was attempted, but prevented the
  failure.}.  The original, non-normalized test case
makes this far less clear, as the use of {\tt CopyFeatures} and the
multiplicity of shapefiles involved disguises the essence of the
problem.

Normalization and generalization are also being used to prepare an
API-behavior regression suite for ArcPy.  One of the challenges of
using a large API like ArcPy is that behavior of the system can change
from version to version.  In some cases this is due to new faults, or
fixed faults, but in other cases there is an undocumented change,
especially for unusual input combinations.  In order to assist ArcPy
developers, we are preparing a test suite that covers as much as
possible of the Python source in the latest version of ArcPy, 10.3,
and records the values returned.  For future versions of ArcPy (or
older versions), a ``semantic diff'' with 10.3, based on these calls,
can be produced by running this suite.  The tests in the suite are
normalized and generalized to help users understand the set of
conditions behind an API usage's behavior.  This ``coverage
regression'' quick test \cite{icst2014} will also be
useful for understanding  the API, since the set of online examples of usage
from Esri is usually limited to a small set of parameter combinations.

\begin{figure}[t]
{\scriptsize 
\begin{code}
shapefile2 = "C:\\arctmp\\new3.shp" 
shapefile1 = "C:\\arctmp\\new3.shp" 
featureclass2 = shapefile2 
featureclass0 = shapefile1 
shapefilelist2 = 
   glob.glob("C:\\Arctmp\\*.shp") 
fieldname0 = "newf3" 
shapefile1 = shapefilelist2 [0] 
featureclass1 = shapefile1 
arcpy.CopyFeatures\_management
   (featureclass1,featureclass2) 
op1 = ">" 
newlayer2 = "l2" 
val1 = "100" 
selectiontype2 = "SWITCH\_SELECTION" 
fieldname1 = "newf1" 
arcpy.MakeFeatureLayer\_management
   (featureclass0, newlayer2) 
arcpy.SelectLayerByAttribute\_management
   (newlayer2,selectiontype2,
   ' "'+fieldname0+'" '+op1+val1) 
op0 = ">" 
arcpy.Delete\_management(featureclass2) 
arcpy.SelectLayerByAttribute\_management
   (newlayer2,selectiontype2,
   ' "'+fieldname1+'" '+op0+val1) 
\end{code}
}
\caption{Original test case for ArcPy (19 steps).}
\label{esriorig}
\end{figure}

\begin{figure}[t]
{\scriptsize 
\begin{code}
shapefilelist0 = 
   glob.glob("C:\\Arctmp\\*.shp")        \textcolor{black!60}{\# STEP 0}
\textcolor{black!60}{\#[}
shapefile0 = shapefilelist0 [0]        \textcolor{black!60}{\# STEP 1}
newlayer0 = "l1"                       \textcolor{black!60}{\# STEP 2}
\textcolor{black!60}{\#  or newlayer0 = "l2" }
\textcolor{black!60}{\#  or newlayer0 = "l3" }
\textcolor{black!60}{\#  swaps with steps 3 4 5 6 7}
\textcolor{black!60}{\#] (steps in [] can be in any order)}
\textcolor{black!60}{\#[}
featureclass0 = shapefile0             \textcolor{black!60}{\# STEP 3}
\textcolor{black!60}{\#  swaps with step 2}
fieldname0 = "newf1"                   \textcolor{black!60}{\# STEP 4}
\textcolor{black!60}{\#  or fieldname0 = "newf2" }
\textcolor{black!60}{\#  or fieldname0 = "newf3" }
\textcolor{black!60}{\#  swaps with steps 2 8}
selectiontype0 = "SWITCH\_SELECTION"    \textcolor{black!60}{\# STEP 5}
\textcolor{black!60}{\#  or selectiontype0 = "NEW\_SELECTION" }
\textcolor{black!60}{\#  or selectiontype0 = "ADD\_TO\_SELECTION" }
\textcolor{black!60}{\#  or selectiontype0 = "REMOVE\_FROM\_SELECTION"}
\textcolor{black!60}{\#  or selectiontype0 = "SUBSET\_SELECTION"}
\textcolor{black!60}{\#  or selectiontype0 = "CLEAR\_SELECTION"   }
\textcolor{black!60}{\#  swaps with steps 2 8}
op0 = ">"                              \textcolor{black!60}{\# STEP 6}
\textcolor{black!60}{\#  or op0 = "<" }
\textcolor{black!60}{\#  swaps with steps 2 8}
val0 = "100"                           \textcolor{black!60}{\# STEP 7}
\textcolor{black!60}{\#  or val0 = "1000" }
\textcolor{black!60}{\#  swaps with steps 2 8}
\textcolor{black!60}{\#] (steps in [] can be in any order)}
arcpy.MakeFeatureLayer\_management
   (featureclass0, newlayer0)          \textcolor{black!60}{\# STEP 8}
\textcolor{black!60}{\#  swaps with steps 4 5 6 7}
arcpy.SelectLayerByAttribute\_management
   (newlayer0,selectiontype0,
   ' "'+fieldname0+'" '+op0+val0)      \textcolor{black!60}{\# STEP 9}
arcpy.Delete\_management(featureclass0) \textcolor{black!60}{\# STEP 10}
arcpy.SelectLayerByAttribute\_management
   (newlayer0,selectiontype0,
   ' "'+ fieldname0+'" '+op0+val0)     \textcolor{black!60}{\# STEP 11}
\end{code}
}
\caption{Normalized and generalized test case for ArcPy 
  (12 steps).}
\label{esrinormgen}
\end{figure}