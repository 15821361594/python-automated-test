\section{Related Work}

There is a vast amount of previous work on automated generation of
tests for (API-based) software systems
\cite{Pacheco,FA11,GodefroidKS05} and random testing in particular
\cite{ICSEDiff,Pacheco,AMFL11,ARTChen,ISSTAART,FASE,HamletOnly,Hamlet94,ClaessenH00,CiupaLOM07,RandFormal,woda08,andrews-etal-rute-rt,ASE08,evalrand,csmith},
some dating back to the early 1980s. It is far beyond the scope of
this paper to explore that literature in detail.  The interested
reader is directed to the cited papers, as well as general surveys of
automated test generation in particular \cite{anand2013orchestrated}
or recent software testing research in general \cite{orsofuse}.  

%This paper relies primarily on past
%results showing that random testing \cite{Hamlet94,Fuzz} can be highly effective for
%finding software faults in large libraries
%\cite{Pacheco,ICSEDiff,RandFormal,CFV08,AMAI}.  To our knowledge, there is almost
%no work on automated/random test generation for GIS in particular, and
%little or no work on allowing non-traditional software developers to apply
%automated (random) test generation to APIs.

%TSTL is a domain-specific language (DSL) \cite{Fow10} for testing, in
%the spirit of the famous QuickCheck tool for Haskell
%\cite{ClaessenH00}; other DSLs for testing exist \cite{UDITA}, but
%these are more limited in scope and function.  

To our knowledge, there has been no previous proposal of a concise
domain-specific-language \cite{Fow10} like TSTL, to assist users in building test
harnesses.  There is limited previous work on
building common frameworks for random testing and model checking
\cite{woda08}, or proposing common terminology for imperative
harnesses \cite{woda12}.  Earlier publications on TSTL itself \cite{NFM15,ISSTA15} presented a
language considerably more limited in functionality and with a more
difficult-to-read syntax.  These publications omitted details of the tools provided in the TSTL distribution for
off-the-shelf testing \cite{tstl}, and provided little practical
guidance to potential users of TSTL.  However, some interesting
details of the current TSTL implementation were presented in the NASA
Formal Methods paper \cite{NFM15} that we omit in
the interest of space (and because implementation details are subject
to change).

There exist various testing tools and languages of a somewhat
different flavor: e.g. Korat \cite{Korat}, which has a much more fixed
input domain specification, or the tools built to support the Next
Generation Air Transportation System (NextGen) software
\cite{TameInputs}.  The closest of these is the UDITA language
\cite{UDITA}, an extension of Java with non-deterministic choice
operators and {\tt assume}, which yields a very different language
that shares our goal.  TSTL aims more at the \emph{generation} of
tests than the \emph{filtering} of tests (as defined in the UDITA
paper), while UDITA supports both approaches.  This goal of UDITA (and
resulting need for first-class {\tt assume} statements) means that it
must be hosted inside a complex (and sometimes non-trivial to
install/use) tool, JPF \cite{JPF2}, rather than generating a
stand-alone simple interface to a test space, as with TSTL.  Building
``UDITA'' for a new language is far more challenging than porting
TSTL.  UDITA supports many fewer constructs to assist harness
development.

The design of the SPIN model checker \cite{SPIN} and its model-driven
extension to include native C code \cite{ModelDriven} inspired the
flavor of TSTL's domain-specific language, though our approach is more
declarative than the ``imperative'' model checker produced by SPIN,
and our system less tied to a particular method of exploration.
Work at JPL on languages for analyzing spacecraft telemetry
logs in testing \cite{scriptstospecs} provided a working example of a
Python-based declarative language useful in testing.  The pool
approach to test case construction is derived from work on canonical
forms and enumeration of unit tests \cite{AndrewsTR}, and common to
some other test generators \cite{Pacheco}.

Some recent work on automated test generation has given more attention
to practical, rather than primarily algorithmic, issues than in the
past; we suspect this shows a growing technological maturity for
automated test case generation.  E.g., the papers by the NASA/JPL
group on testing the Curiosity rover's file system
\cite{ICSEDiff,CFV08,AMAI} have a largely practical focus, and the
work of Lei and Andrews \cite{MinUnit} emphasizes the need for
delta-debugging in realistic random testing.  We found that test case
readability was important in our efforts, and recent academic work
\cite{Readable,Guava} has introduced principled methods for improving
the readability of test cases for humans, even though this does not
improve fault detection, coverage, or other traditional measures of
test effectiveness.

The literature on testing GIS (Geographic Information System) software
in particular seems to consist of one paper proposing a very limited
application of automated testing to assist GIS users, primarily in
model development \cite{GISTest}.  That work does not target the
reliability or correctness of the underlying GIS engine, or GIS
libraries.  There is also some discussion of automated testing for GIS
in various blog posts and discussion groups (e.g.,
\cite{gisblog1,gisblog2}), but no formal academic case studies.  These
discussions also tend to focus on application testing or GUI testing,
rather than testing of library code used across GIS applications.
There is a simple extension to Python unit testing modules for the
GRASS open source GIS system \cite{GRASSunit}, but this does not
provide any automated test generation.

There is a significant body of work on end-user testing of software,
part of the larger field of end-user software engineering
\cite{burnettEUSE,Silos}.  End-user software engineering examines how
software can best be produced by developers who do not have a
traditional computer science background, and are often primarily
interested in an application of programming, rather than software
development as a profession.  GIS developers are (we believe) a
typical example \cite{Segal07}:  they are technically skilled  individuals whose
primary expertise is not in software development, but who, in order to
pursue their goals, must develop, maintain, and test significant
software systems.

The earliest work focusing on software testing for
end-user software engineers explored how to test spreadsheets
\cite{rothermelTOSEM,rothermel2000wysiwyt}.  Other work has focused on
errors end-users make in specifying systems \cite{Phalgune}, and how
end-users of machine learning systems (who may be machine learning
experts, or individuals with no programming knowledge at all) can test
such systems \cite{OnlyOracle,kulesza-eud11,shinsel-vlhcc}.  To our
knowledge, no previous work considers function call sequence testing for
end-users.  Previous work on such testing has often been performed
only by software testing researchers, not even including traditional developers.

%Chen et al. propose metamorphic testing
%\cite{MetaTest,isstamorph,metamorph,chentest} as a possible solution
%to the oracle problem (defining correct behavior of a software system)
%for end-user programmers \cite{MetamorphEndUser}.  In metamorphic
%testing, while the correct output of a program is not known, the
%effect of certain modifications to an input is known (e.g., increasing
%a certain input should increase a certain output), allowing faults to
%be detected even without a definition of the correct result.  